# -*- coding: utf-8 -*-
"""RAG_SYSTEM_CODE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkeml3BNYiTEs3Eipr6m7IwAalAx-qwv
"""

### since we will be working with pdf we have to install the "pypdf"

!pip install pypdf
!pip install pypdf2
!pip install transformers
!pip install langchain
!pip install sentence_transformers
!pip install llama_index
!pip install llama-index-llms-huggingface
!pip install huggingface_hub
!pip install -U langchain-community
!pip install llama-index-embeddings-langchain
!pip install langchain
!pip install datasets
!pip install fitz
!pip install docx2txt
!pip install -q transformers einops accelerate langchain bitsandbytes
!pip install gradio

# Check current disk usage
!df -h

# List files in the root directory
!ls -lh /

# List files in the /opt/bin/.nvidia directory
!ls -lh /opt/bin/.nvidia

# List files in the /content directory (where Colab typically stores user files)
!ls -lh /content

# Remove unnecessary files and directories (uncomment and modify as needed)
# For example, if there are large files in /content, you can remove them:
# !rm -rf /content/your-unnecessary-file-or-directory

# Clear pip cache
!rm -rf ~/.cache/pip

# Clear apt cache
!sudo apt-get clean

# Clear temporary files in /tmp directory
!rm -rf /tmp/*

# Clear other unnecessary caches or temporary files (adjust paths as necessary)
!rm -rf /root/.cache
!rm -rf /var/lib/apt/lists/*

# Check disk usage again to see the changes
!df -h

import os
import warnings
import torch
import gradio as gr
from huggingface_hub import login
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, Document
from llama_index.core import PromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from llama_index.embeddings.langchain import LangchainEmbedding

warnings.filterwarnings('ignore')

# Login to HuggingFace
def login_huggingface(token):
    login(token)

# Load data from the uploaded file
def load_data(directory):
    document = SimpleDirectoryReader(directory).load_data()
    return document

# Initialize the LLM
def initialize_llm(system_prompt, query_template, context_template):
    query_wrapper_prompt = PromptTemplate(query_template)
    context_template_prompt = PromptTemplate(context_template)

    llm = HuggingFaceLLM(
        context_window=4096,
        max_new_tokens=256,
        generate_kwargs={"temperature": 0.0, "do_sample": False},
        system_prompt=system_prompt,
        query_wrapper_prompt=query_wrapper_prompt,
        tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
        model_name="meta-llama/Llama-2-7b-chat-hf",
        device_map="auto",
        model_kwargs={"torch_dtype": torch.float16, "load_in_8bit": True}
    )

    return llm

# Initialize the embedding model
def initialize_embedding_model():
    embed_model = LangchainEmbedding(
        HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
    )
    return embed_model

# Create service context
def create_service_context(llm, embed_model, chunk_size=1024):
    service_context = ServiceContext.from_defaults(
        chunk_size=chunk_size,
        llm=llm,
        embed_model=embed_model
    )
    return service_context

# Create vector store index
def create_vector_store_index(document, service_context):
    index = VectorStoreIndex.from_documents(document, service_context=service_context, show_progress=True)
    return index

# Query the index
def query_index(index, question):
    query_engine = index.as_query_engine()
    response = query_engine.query(question)
    return response

# Main function to process the input document and query
def process_input(file_path, question):
    token = "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"  # Replace with your HuggingFace token
    login_huggingface(token)

    # Print the file path
    print(f"Uploaded file path: {file_path}")

    # Pass the directory containing the file to load_data
    document = load_data(os.path.dirname(file_path))

    system_prompt = 'You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.'
    query_template = "\n" + system_prompt + "</s>\n{query_str}</s>"
    context_template = "We have provided context information below. \n\n{context_str}\n\nGiven this information, please answer the question: {query_str}\n"

    llm = initialize_llm(system_prompt, query_template, context_template)

    embed_model = initialize_embedding_model()

    service_context = create_service_context(llm, embed_model)

    index = create_vector_store_index(document, service_context)

    response = query_index(index, question)

    return response

# Create the Gradio interface
def create_gradio_interface():
    iface = gr.Interface(
        fn=process_input,
        inputs=[
            gr.File(label="Upload Document", type="filepath"),
            gr.Textbox(label="Enter your question")
        ],
        outputs="text",
        title="Document-based Q&A System",
        description="Upload a document and enter your question. The system will provide an answer based on the document."
    )
    return iface

if __name__ == "__main__":
    iface = create_gradio_interface()
    iface.launch(share=True, debug=True)

