# -*- coding: utf-8 -*-
"""RAG_SYSTEM_CODE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkeml3BNYiTEs3Eipr6m7IwAalAx-qwv
"""

### since we will be working with pdf we have to install the "pypdf"

!pip install pypdf
!pip install pypdf2
!pip install transformers
!pip install langchain
!pip install sentence_transformers
!pip install llama_index
!pip install llama-index-llms-huggingface
!pip install huggingface_hub
!pip install -U langchain-community
!pip install llama-index-embeddings-langchain
!pip install langchain
!pip install datasets
!pip install fitz
!pip install docx2txt
!pip install -q transformers einops accelerate langchain bitsandbytes
!pip install gradio



import os
import warnings
import torch
import gradio as gr
from huggingface_hub import login
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, Document
from llama_index.core import PromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from llama_index.embeddings.langchain import LangchainEmbedding

warnings.filterwarnings('ignore')

# Login to HuggingFace
def login_huggingface(token):
    try:
        login(token)
    except Exception as e:
        print(f"Error logging in to HuggingFace: {e}")
        return None

# Load data from the uploaded file
def load_data(directory):
    try:
        document = SimpleDirectoryReader(directory).load_data()
        return document
    except Exception as e:
        print(f"Error loading data from {directory}: {e}")
        return None

# Initialize the LLM
def initialize_llm(system_prompt, query_template, context_template):
    try:
        query_wrapper_prompt = PromptTemplate(query_template)
        context_template_prompt = PromptTemplate(context_template)

        llm = HuggingFaceLLM(
            context_window=4096,
            max_new_tokens=256,
            generate_kwargs={"temperature": 0.0, "do_sample": False},
            system_prompt=system_prompt,
            query_wrapper_prompt=query_wrapper_prompt,
            tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
            model_name="meta-llama/Llama-2-7b-chat-hf",
            device_map="auto",
            model_kwargs={"torch_dtype": torch.float16, "load_in_8bit": True}
        )

        return llm
    except Exception as e:
        print(f"Error initializing LLM: {e}")
        return None

# Initialize the embedding model
def initialize_embedding_model():
    try:
        embed_model = LangchainEmbedding(
            HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
        )
        return embed_model
    except Exception as e:
        print(f"Error initializing embedding model: {e}")
        return None

# Create service context
def create_service_context(llm, embed_model, chunk_size=1024):
    try:
        service_context = ServiceContext.from_defaults(
            chunk_size=chunk_size,
            llm=llm,
            embed_model=embed_model
        )
        return service_context
    except Exception as e:
        print(f"Error creating service context: {e}")
        return None

# Create vector store index
def create_vector_store_index(document, service_context):
    try:
        index = VectorStoreIndex.from_documents(document, service_context=service_context, show_progress=True)
        return index
    except Exception as e:
        print(f"Error creating vector store index: {e}")
        return None

# Query the index
def query_index(index, question):
    try:
        query_engine = index.as_query_engine()
        response = query_engine.query(question)
        return response
    except Exception as e:
        print(f"Error querying index: {e}")
        return None

# Main function to process the input document and query
def process_input(file_path, question):
    token = "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"  # Replace with your HuggingFace token
    login_huggingface(token)

    # Print the file path
    print(f"Uploaded file path: {file_path}")

    # Pass the directory containing the file to load_data
    document = load_data(os.path.dirname(file_path))
    if document is None:
        return "Error: Unable to load document."

    system_prompt = 'You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.'
    query_template = "\n" + system_prompt + "</s>\n{query_str}</s>"
    context_template = "We have provided context information below. \n\n{context_str}\n\nGiven this information, please answer the question: {query_str}\n"

    llm = initialize_llm(system_prompt, query_template, context_template)
    if llm is None:
        return "Error: Unable to initialize the language model."

    embed_model = initialize_embedding_model()
    if embed_model is None:
        return "Error: Unable to initialize the embedding model."

    service_context = create_service_context(llm, embed_model)
    if service_context is None:
        return "Error: Unable to create service context."

    index = create_vector_store_index(document, service_context)
    if index is None:
        return "Error: Unable to create vector store index."

    response = query_index(index, question)
    if response is None:
        return "Error: Unable to get a response from the index."

    return response

# Create the Gradio interface
def create_gradio_interface():
    iface = gr.Interface(
        fn=process_input,
        inputs=[
            gr.File(label="Upload Document", type="filepath"),
            gr.Textbox(label="Enter your question")
        ],
        outputs="text",
        title="Document-based Q&A System",
        description="Upload a document and enter your question. The system will provide an answer based on the document."
    )
    return iface

if __name__ == "__main__":
    iface = create_gradio_interface()
    iface.launch(share=True, debug=True)
